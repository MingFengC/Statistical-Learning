---
title: "Linear Regression ISLR"
author: "Chua Ming Feng"
date: "2022-09-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Applied Questions ISLR Linear Regression

## Q8a

```{r}
library(ISLR2)
library(fastDummies)
```

```{r}
y <- Auto$mpg
x <- Auto$horsepower
lm.fit <- lm(y ~ x)
summary(lm.fit)

xnew <- data.frame(x = 98)
confint(lm.fit, level = 0.95)
predict(lm.fit, newdata = xnew, interval = "prediction")
```

We observe that the p-value < 2e-16 < 0.05, thus we can reject H0 conclude that the predictor and response do have a relationship.
The relationship is strong as the p-value < 2.2e-16 < 0.05
The relationship between the predictor and response is negative

## Q8b

```{r}
plot(x, y)
abline(lm.fit)
plot(lm.fit)
```

# Q9a

```{r}
pairs(Auto)
```

## Q9b

```{r}
Auto.new <- Auto[, -ncol(Auto)]
Auto.d <- dummy_cols(Auto.new, select_columns = 'origin', remove_selected_columns = TRUE)
cor(Auto.d)
```

## Q9c

```{r}
Auto.new$origin <- as.factor(Auto.new$origin)
lm.fit <- lm(mpg ~ ., data = Auto.new)
summary(lm.fit)
```

Yes since the p-value of the F-statistic is extremely small.
Significant predictors: Displacement, weight, year and origin.
It suggests that an increase of 1 model year increases the mpg by 77700.

## Q9D

```{r}
plot(lm.fit)
```

From the residuals vs fitted plot, we observe slight non-linearity which could mean non-linearity in the data, most likely a quadratic trend. From the Q-Q plot, we observe 1 possible outlier observation 323. From the scale-location plot, we see that the variance of the residuals increases as the fitted values increases. From the leverage plot, we can see that observation 14 has significantly higher leverage than the other points.

## Q9e

```{r}
lm.fit.int <- lm(mpg ~ .^2, data = Auto.new)
summary(lm.fit.int)
```

cylinders:acceleration and acceleration:year

## Q9f

```{r}
lm.fit.log <- lm(sqrt(mpg) ~ . + I(horsepower^2), data = Auto.new)
summary(lm.fit.log)
plot(lm.fit.log)
```

We can perform a sqrt transformation on mpg to alleviate the issue where variance increases as fitted values increases. We can also apply a quadratic transformation to horsepower as there is a quadratic relationship from the scatterplot.

## Q10a

```{r}
lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(lm.fit)
plot(lm.fit)
```

## Q10b

Since UrbanYes has a negative coefficient, an urban store has fewer sales as compared to rural stores, however the difference is not significant as the p-value is larger than 0.05.

Since USYes has a positive coefficient, a US store has higher sales as compared to stores which are not in the US, the difference is significant as the p-value is small.

## Q10c

Sales = -0.054459Price -0.021916Urban + 1.200573US, where Urban = 1 if store is urban and US = 1 if store is in the US

## Q10d

```{r}
lm.fit <- lm(Sales ~ ., data = Carseats)
summary(lm.fit)
```

We can reject CompPRice, Income, Advertising, Price, Shelve and age.

## Q10e

```{r}
lm.fit <- lm(Sales ~ Population + Education + Urban + US, data = Carseats)
summary(lm.fit)
plot(lm.fit)
```

From a, the R2 is 0.2393 while the R2 from e is 0.03465. The model from a is able to explain 23.93% of the variation seen in the Sales while the model from e is only able to explain 3.465% of the variation seen in Sales.

## Q10g

```{r}
confint(lm.fit)
```

## Q10h

```{r}
plot(lm.fit)
```

We observe that there are no obvious outliers or points with high leverage.

# Q11a

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

lm.fit <- lm(y ~ x + 0)
summary(lm.fit)
```

y has significant relationship with x.

## Q11b

```{r}
lm.fit <- lm(x ~ y + 0)
summary(lm.fit)
```

x has significant relationship with y.

## Q13a

```{r}
set.seed(1)
x <- rnorm(100)
```

## Q13b

```{r}
eps <- rnorm(100, sd = 0.5)
```

## Q13c

```{r}
y <- -1 + 0.5 * x + eps
length(y)
```

beta0 = -1, beta1 = 0.5

## Q13d

```{r}
plot(x, y)
```

We can observe a positive linear relationship between y and x. As x increases, y increases.

## Q13e

```{r}
lm.fit <- lm(y ~ x)
summary(lm.fit)
confint(lm.fit)
```

beta0 = -1.01885 = -1 approximately
beta1 = 0.49947 = 0.5 approximately

We see that the given coefficients were sufficiently close to the fitted least square coefficients.

## Q13f

```{r}
plot(x, y)
abline(lm.fit, col = "blue")
abline(a = -1, b = 0.5, col = "red")
legend("bottomright", legend = c("Fitted regression line", "Population regression line"), pch = "/", col = c("blue", "red"))
```

## Q13g

```{r}
lm.fit <- lm(y ~ x + I(x^2))
summary(lm.fit)
plot(lm.fit)
```

The first R2 was 0.4674 and the new R2 is 0.4779. There is a slight increase in R2. Addition of the term improves the fit slightly.

## Q13h

```{r}
eps <- rnorm(100, sd = 0.1)
y <- -1 + 0.5 * x + eps
lm.fit <- lm(y ~ x)
summary(lm.fit)
plot(x, y)
abline(lm.fit, col = "blue")
abline(a = -1, b = 0.5, col = "red")
legend("bottomright", legend = c("Fitted regression line", "Population regression line"), pch = "/", col = c("blue", "red"))
lm.fit <- lm(y ~ x + I(x^2))
summary(lm.fit)
plot(lm.fit)
confint(lm.fit)
```

## Q13i

```{r}
eps <- rnorm(100, sd = 0.5)
y <- -1 + 0.5 * x + eps
lm.fit <- lm(y ~ x)
summary(lm.fit)
plot(x, y)
abline(lm.fit, col = "blue")
abline(a = -1, b = 0.5, col = "red")
legend("bottomright", legend = c("Fitted regression line", "Population regression line"), pch = "/", col = c("blue", "red"))
lm.fit <- lm(y ~ x + I(x^2))
summary(lm.fit)
plot(lm.fit)
confint(lm.fit)
```

## Q14a

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

beta0 = 2, beta1 = 2, beta2 = 0.3

## Q14b

```{r}
cor(x1, x2)
plot(x1, x2)
```

## Q14c

```{r}
lm.fit <- lm(y ~ x1 + x2)
summary(lm.fit)
plot(lm.fit)
```

beta0 = 2.1305, beta1 = 1.4396, beta2 = 1.0097

The coefficient for the intercept is approximately the same at 2 but the difference of beta1 and beta2 are large. 
We can reject the null hypothesis for beta1 as the p-value is small. 

## Q14d

```{r}
lm.fit <- lm(y ~ x1)
summary(lm.fit)
plot(lm.fit)
```

Yes we can reject the null hypothesis.

## Q14e

```{r}
lm.fit <- lm(y ~ x2)
summary(lm.fit)
plot(lm.fit)
```

We can reject the null hypothesis.

## Q14f

No the results are not contradictory. This is because both predictors are correlated, so when x2 was added to the model that already contains x1, it essentially is not contributing new information, which means x2 is redundant when using both predictors.

## Q14g

```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
lm.fit <- lm(y ~ x1 + x2)
summary(lm.fit)
plot(lm.fit)
```

```{r}
lm.fit <- lm(y ~ x1)
summary(lm.fit)
plot(lm.fit)
```

```{r}
lm.fit <- lm(y ~ x2)
summary(lm.fit)
plot(lm.fit)
```

In the models containing x2, the point has high leverage. This point is not an outlier in all the models.

## Q15a

```{r}
for (i in 2:length(Boston)) {
  x <- Boston[, i]
  print(colnames(Boston)[i])
  lm.fit <- lm(Boston$crim ~ x, data = Boston)
  print(summary(lm.fit))
}
```

All predictors are significant except for chas which has a p-value of 0.209.

## Q15b

```{r}
lm.fit <- lm(crim ~ ., data = Boston)
summary(lm.fit)
plot(lm.fit)
```

For the predictors zn, dis, rad and medv we can reject the null hypothesis as their p-values are small.

## Q15d

```{r}
for (i in 2:length(Boston)) {
  x <- Boston[, i]
  print(colnames(Boston)[i])
  lm.fit <- lm(Boston$crim ~ x + I(x^2) + I(x^3), data = Boston)
  print(summary(lm.fit))
}
```











